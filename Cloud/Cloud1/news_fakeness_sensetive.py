import csv
from ollama import chat

LIMIT = 1000
INPUT_FILE = "posts_groundtruth.txt"
OUTPUT_FILE = "result_with_sentiment.csv"

def load_dataset(file_path, limit=None):
    dataset = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for i, row in enumerate(reader):
            if limit and i >= limit:
                break
            dataset.append({
                "post_id": row["post_id"],
                "post_text": row["post_text"],
                "label": row["label"].strip().lower()
            })
    return dataset

def analyze_sentiment(text):
    prompt = f"""
è¯·ä½ åˆ¤æ–­ä¸‹é¢è¿™æ®µç¤¾äº¤åª’ä½“å†…å®¹çš„æƒ…æ„Ÿå€¾å‘ï¼Œåªèƒ½å›ç­”â€œç§¯æâ€ã€â€œä¸­æ€§â€æˆ–â€œæ¶ˆæâ€ä¸‰ç§ä¹‹ä¸€ï¼Œä¸è¦è¾“å‡ºè§£é‡Šï¼š

å†…å®¹ï¼š
{text}
"""
    try:
        response = chat(model='gemma3:12b-it-qat', messages=[{
            'role': 'user',
            'content': prompt.strip()
        }])
        reply = response['message']['content'].strip()
        if "ç§¯æ" in reply:
            return "ç§¯æ"
        elif "æ¶ˆæ" in reply:
            return "æ¶ˆæ"
        elif "ä¸­æ€§" in reply:
            return "ä¸­æ€§"
        else:
            return "æœªçŸ¥"
    except Exception as e:
        print(f"[æƒ…æ„Ÿåˆ†æé”™è¯¯] {e}")
        return "error"

def predict_fakeness(text, sentiment=None):
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ–°é—»çœŸä¼ªåˆ¤åˆ«ä¸“å®¶ã€‚è¯·æ ¹æ®ä¸‹é¢çš„å†…å®¹åˆ¤æ–­å®ƒæ˜¯â€œçœŸæ–°é—»â€è¿˜æ˜¯â€œå‡æ–°é—»â€ã€‚ä½ åªèƒ½å›ç­”â€œçœŸâ€æˆ–â€œå‡â€ï¼Œä¸èƒ½è¾“å‡ºå…¶å®ƒå†…å®¹æˆ–è§£é‡Šã€‚

è¯·æ³¨æ„ï¼š
1ã€â€œå‡æ–°é—»â€æŒ‡çš„æ˜¯è™šæ„ã€æ­ªæ›²æˆ–è¯¯å¯¼æ€§çš„ä¿¡æ¯ï¼Œ
2ã€â€œçœŸæ–°é—»â€æŒ‡çš„æ˜¯åŸºäºäº‹å®ã€å¯ä»¥éªŒè¯çš„ä¿¡æ¯ã€‚
3ã€è¯·æ ¹æ®æƒ…æ„Ÿåˆ†æç»“æœä¸æ–°é—»å†…å®¹æœ¬èº«è¿›è¡Œåˆ¤æ–­ã€‚

æƒ…æ„Ÿåˆ†æç»“æœï¼š{sentiment}

ç°åœ¨è¯·åˆ¤æ–­ä»¥ä¸‹å†…å®¹ï¼š

{text}
"""
    try:
        response = chat(model='gemma3:12b-it-qat', messages=[{
            'role': 'user',
            'content': prompt.strip()
        }])
        reply = response['message']['content'].strip()
        print(f"æ¨¡å‹å›å¤: {reply}")
        if reply == "çœŸ":
            return "real"
        elif reply == "å‡":
            return "fake"
        else:
            return "unknown"
    except Exception as e:
        print(f"[çœŸå‡åˆ¤æ–­é”™è¯¯] {e}")
        return "error"

def run_analysis(data, save_path=OUTPUT_FILE):
    results = []

    for i, entry in enumerate(data):
        post_id = entry["post_id"]
        text = entry["post_text"]
        true_label = entry["label"]

        sentiment = analyze_sentiment(text)
        pred_label = predict_fakeness(text, sentiment=sentiment)

        is_correct = pred_label == true_label
        symbol = "âœ…" if is_correct else "âŒ"
        print(f"[{i+1}/{len(data)}] Pred: {pred_label}, True: {true_label} Sentiment: {sentiment} {symbol}")

        results.append({
            "post_id": post_id,
            "post_text": text,
            "true_label": true_label,
            "predicted": pred_label,
            "correct": is_correct,
            "sentiment": sentiment
        })

    correct = sum(1 for r in results if r['correct'])
    total = len(results)

    fake_total = sum(1 for r in results if r['true_label'] == 'fake')
    fake_correct = sum(1 for r in results if r['true_label'] == 'fake' and r['predicted'] == 'fake')

    real_total = sum(1 for r in results if r['true_label'] == 'real')
    real_correct = sum(1 for r in results if r['true_label'] == 'real' and r['predicted'] == 'real')

    acc = correct / total * 100
    acc_fake = fake_correct / fake_total * 100 if fake_total else 0
    acc_real = real_correct / real_total * 100 if real_total else 0

    print("\nğŸ“Š å‡†ç¡®ç‡ç»Ÿè®¡ç»“æœï¼š")
    print(f"âœ… Accuracy       : {acc:.2f}%")
    print(f"ğŸ§ª Accuracy_fake  : {acc_fake:.2f}%")
    print(f"ğŸ“¢ Accuracy_true  : {acc_real:.2f}%")

    with open(save_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=[
            "post_id", "post_text", "true_label", "predicted", "correct", "sentiment"
        ])
        writer.writeheader()
        writer.writerows(results)

    print(f"ğŸ“„ å·²ä¿å­˜å¸¦æƒ…æ„Ÿåˆ†æçš„ç»“æœåˆ°: {save_path}")

if __name__ == "__main__":
    dataset = load_dataset(INPUT_FILE, limit=LIMIT)
    run_analysis(dataset)
