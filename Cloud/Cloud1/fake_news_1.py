import csv
from ollama import chat

#å¯¹æ•°æ®é›†çš„è¯»å–æ•°é‡è¿›è¡Œé™åˆ¶
LIMIT = 1000
INPUT_FILE = "posts_groundtruth.txt"
OUTPUT_FILE = "result_no_sentiment.csv"

#è¯»å–æ•°æ®ï¼Œå¹¶æå–å‡ºå…¶ä¸­æœ‰ç”¨çš„éƒ¨åˆ†
def load_dataset(file_path, limit=None):
    dataset = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for i, row in enumerate(reader):
            if limit and i >= limit:
                break
            dataset.append({
                "post_id": row["post_id"],
                "post_text": row["post_text"],
                "label": row["label"].strip().lower()
            })
    return dataset
#prompt
def predict_fakeness(text):
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ–°é—»çœŸä¼ªåˆ¤åˆ«ä¸“å®¶ã€‚è¯·æ ¹æ®ä¸‹é¢çš„å†…å®¹åˆ¤æ–­å®ƒæ˜¯â€œçœŸæ–°é—»â€è¿˜æ˜¯â€œå‡æ–°é—»â€ã€‚ä½ åªèƒ½å›ç­”â€œçœŸâ€æˆ–â€œå‡â€ï¼Œä¸èƒ½è¾“å‡ºå…¶å®ƒå†…å®¹æˆ–è§£é‡Šã€‚

è¯·æ³¨æ„ï¼šâ€œå‡æ–°é—»â€æŒ‡çš„æ˜¯è™šæ„ã€æ­ªæ›²æˆ–è¯¯å¯¼æ€§çš„ä¿¡æ¯ï¼Œè€Œâ€œçœŸæ–°é—»â€æŒ‡çš„æ˜¯åŸºäºäº‹å®ã€å¯ä»¥éªŒè¯çš„ä¿¡æ¯ã€‚

ç°åœ¨è¯·åˆ¤æ–­ä»¥ä¸‹å†…å®¹ï¼š

å†…å®¹ï¼š
"{text}"

"""
#åŒ¹é…å¤§æ¨¡å‹åˆ¤æ–­ï¼Œå¹¶è¾“å‡ºå¤§æ¨¡å‹åˆ¤æ–­ç»“æœ
    try:
        response = chat(model='gemma3:12b-it-qat', messages=[{
            'role': 'user',
            'content': prompt.strip()
        }])
        reply = response['message']['content'].strip()
        print(f"æ¨¡å‹å›å¤: {reply}")
        if reply == "çœŸ":
            return "real"
        elif reply == "å‡":
            return "fake"
        else:
            return "unknown"

    except Exception as e:
        print(f"[Error] {e}")
        return "error"


#å°†å¤§æ¨¡å‹çš„è¿è¡Œè¾“å‡ºç»“æœä¸æ•°æ®é›†ä¸­çš„çœŸå‡è¿›è¡ŒåŒ¹é…ï¼Œåˆ¤æ–­å¤§æ¨¡å‹çš„å›ç­”æ˜¯å¦æ­£ç¡®
def run_analysis(data, save_path=OUTPUT_FILE, save_prefix='no_sentiment'):
    results = []

    for i, entry in enumerate(data):
        post_id = entry["post_id"]
        text = entry["post_text"]
        true_label = entry["label"]

        pred_label = predict_fakeness(text)
        is_correct = pred_label == true_label
        symbol = "âœ…" if is_correct else "âŒ"
        print(f"[{i+1}/{len(data)}] Pred: {pred_label}, True: {true_label} {symbol}")

        results.append({
            "post_id": post_id,
            "post_text": text,
            "true_label": true_label,
            "predicted": pred_label,
            "correct": is_correct
        })

#è®¡ç®—å¤§æ¨¡å‹å‡†ç¡®ç‡
    total = len(results)
    correct = sum(1 for r in results if r['correct'])

    fake_total = sum(1 for r in results if r['true_label'] == 'fake')
    fake_correct = sum(1 for r in results if r['true_label'] == 'fake' and r['predicted'] == 'fake')

    real_total = sum(1 for r in results if r['true_label'] == 'real')
    real_correct = sum(1 for r in results if r['true_label'] == 'real' and r['predicted'] == 'real')

    acc = correct / total * 100
    acc_fake = fake_correct / fake_total * 100 if fake_total else 0
    acc_real = real_correct / real_total * 100 if real_total else 0

    print("\nğŸ“Š å‡†ç¡®ç‡ç»Ÿè®¡ç»“æœï¼š")
    print(f"âœ… Accuracy       : {acc:.2f}%")
    print(f"ğŸ§ª Accuracy_fake  : {acc_fake:.2f}%")
    print(f"ğŸ“¢ Accuracy_true  : {acc_real:.2f}%")

# å°†è¿è¡Œç»“æœä¿å­˜ä¸ºresult_no_sentiment.csv
    with open(save_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=[
            "post_id", "post_text", "true_label", "predicted", "correct"
        ])
        writer.writeheader()
        writer.writerows(results)


if __name__ == "__main__":
    dataset = load_dataset(INPUT_FILE, limit=LIMIT)
    run_analysis(dataset)
